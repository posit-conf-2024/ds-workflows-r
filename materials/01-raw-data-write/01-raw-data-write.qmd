---
title: "Raw Data Download"
format:
  email:
    table-of-contents: true
    anchor-sections: true
    code-fold: false
    code-overflow: wrap
    code-summary: "Show Code"
    code-tools: true
    code-link: true
editor: visual
editor_options: 
  chunk_output_type: console
  canonical: true
---

------------------------------------------------------------------------

## General Workshop Activity Notes

Welcome to the first Activity! A few points relevant to this, and all other Activities in the workshop:

1.  The Activities are Quarto documents, presented by default in the Visual Editor for readability
2.  The intention is to run through the document from top to bottom, running each code chunk interactively rather than rendering the document, however, these are also "production" documents that can be deployed to generate our project's production assets
3.  Run the chunk by either clicking the green Play Button in the chunk's top right corner, or use the cmd+enter keyboard shortcut to run the current line or selected text
4.  We'll work through these activities mostly as a group, code-along style because our focus in this workshop is less about code development and more about around processes, tools, and learning concepts. We can best teach this if we do it together!
5.  **Where's my output?** Are you a `Chunk Output Inline` kind of person? Sorry. For this activity, it's set by default to go the console because the HTML renderings from Pointblank get very squished otherwise. It might be like this for other activities too, so don't get surprised üòâ

------------------------------------------------------------------------

## Goals

The goals of this activity are to:

-   Use `{httr2}` to download the raw Washington State Ferries data and weather data, which is used throughout this Workshop
-   Write the raw data to a database
-   Learn some general logging tips to make your scheduled reports more useful
-   Use a simple threshold alert to trigger a notification email
-   Publish this notebook to Posit Connect and put it on a rendering schedule so database updates happen automatically

This will give you experience setting up a repeatable workflow for populating production data sources.

## Data sources

We are using data sets from two main sources

### From WSDOT Traveler Information API

(<https://wsdot.wa.gov/traffic/api/>)

| Endpoint | Description | API |
|------------------------|------------------------|------------------------|
| **Vessel verbose** | Details about each ferry in the fleet, including name, model, and capacity | `https://www.wsdot.wa.gov/ferries/api/vessels/ rest/vesselverbose?apiaccesscode={WSDOT_ACCESS_CODE}` |
| **Vessel history** | Historical sailings, including scheduled actual departure time | `https://www.wsdot.wa.gov/ferries/api/vessels/ rest/vesselhistory/{VESSELNAME}/{DATESTART}/{DATEEND}?apiaccesscode={WSDOT_ACCESS_CODE}` |
| **Terminal locations** | Terminal names and locations, including latitude and longitude | `https://www.wsdot.wa.gov/ferries/api/terminals/ rest/terminallocations?apiaccesscode={WSDOT_ACCESS_CODE}` |

### From Open Meteo

([https://open-meteo.com/en/docs/](https://open-meteo.com/en/docs/historical-weather-api/))

| Endpoint | Description | API |
|------------------------|------------------------|------------------------|
| **Historical weather** | Historical hourly weather at a specified latitude and longitude over a date range | `https://archive-api.open-meteo.com/v1/ archive?{params}` |

We will use the `{httr2}` package to download the data from these API sources.

## Task 1 - Get and set WSDOT environment variable

üîÑ Task

We are using an environment variable to access the WSDOT API called **`WSDOT_ACCESS_CODE`**. Visit <https://wsdot.wa.gov/traffic/api/> to get an API key for the use during this workshop.

To save this environment variable in your local development environment, add it to your `.Renviron` file. To do this, run the following command in your R console:

```{r}
#| eval: false
usethis::edit_r_environ("project")
```

``` {.bash filename=".Renviron"}
# file: either ~/.Renviron or .Renviron in the project root
WSDOT_ACCESS_CODE=PASTE_VALUE_HERE
```

**Don't forget to restart your session to load your revised environment.**

üí° Remember, the `.Renviron` file does not get deployed with this content, and you should not commit it to git. It is a local file that sets environment variables for your R session. You will be able to set this variable in the "Vars" pane on Connect when you deploy.

Now let's set sail!

## Load Packages

```{r}
#| label: setup
 
library(httr2)
library(tidyverse)
library(janitor)
library(DBI)
library(odbc)
library(glue)
library(usethis)
library(ferryfairy)

```

## Task 2 - Download vessel verbose data

üîÑ Task

Get the following data set:

-   **Vessel Verbose**: the `https://www.wsdot.wa.gov/Ferries/API/Vessels/rest/vesselverbose` endpoint contains data about the ferries in the fleet.

Use `{httr2}` to download the vessel verbose data set from the WSDOT API.

This API takes the form:

`https://www.wsdot.wa.gov/ferries/api/vessels/rest/vesselverbose?apiaccesscode={WSDOT_ACCESS_CODE}`

With `{httr2}` we compose the API request as `<base url>/<path (aka endpoint)>/<query parameters>`

üí° Don't split hairs over what part of your string is captured as the base URL versus the path. Do what makes you happy.

```{r}
#| label: download raw vessel data
 
base_url <- "https://www.wsdot.wa.gov"

endpoint <- "ferries/api/vessels/rest/vesselverbose"

# Compose the API request 
req <- request(base_url) |> 
  req_url_path_append(endpoint) |> 
  req_url_query(apiaccesscode = Sys.getenv("WSDOT_ACCESS_CODE")) 

# We can inspect the request before performing it
req
  
# perform the request
response <- req |> 
  req_perform()
response

# convert the body of the response to a tibble
response_body <- response |> 
  resp_body_string() |> 
  jsonlite::fromJSON() |> 
  as_tibble()
response_body

# Sometimes special characters or spaces in column names are not
# database-friendly, so we will use `janitor::clean_names` to
# clean column names so there are no issues writing the data.
# Additionally, the results include a nested dataframe of Class information - 
# we will unnest this so there are no errors with our database interpreting this.
vesselinfo_raw <- response_body |> unnest(Class) |>  clean_names() 
vesselinfo_raw
```

## Task 3 - Connect to the database

üîÑ Task

Create a database connection using the database connection details defined for you by your IT Admin (Workshop Instructors). Avoid hard-coding in any credentials. We're using environment variables here.

We discussed that a database connection can be made using the generic `odbc` package, or a database-specific R package, such as `RPostgres`. The `RPostgres` package is DBI-compliant and is built specifically for PosgreSQL databases. Performance may be faster (particularly for writing data!) than using the generic `odbc` package, and there are more translations available, meaning more dplyr verbs will be available. *However,* because we are in a learning environment for this Workshop, we are choosing to use the `odbc` package so that we can see the database and it's table appear in the RStudio Connections pane. A quality of life tradeoff, but if you're doing intensive database work back in the real world, we recommend using a database-specific package, if available.

```{r}
#| label: make database connection

# Run this code as-is 
con <- dbConnect(
  odbc::odbc(),
  Driver      = "postgresql",
  Server      = Sys.getenv("DATABASE_HOST"),
  Port        = "5432",
  Database    = Sys.getenv("DATABASE_NAME_R"),
  UID         = Sys.getenv("DATABASE_USER_R"),
  PWD         = Sys.getenv("DATABASE_PASSWORD_R"),
  timeout     = 10
)
con

```

::: {.callout-tip icon="false"}
## Heads Up

If you're working through this document in the Workshop, pause here.
:::

## Task 4 - Write to the database

üîÑ Task

-   Use `dbWriteTable` to write `vesselinfo_raw` into the database

```{r}
#| label: database write

my_username <- "_____" #Fill in your username, workshop participants!

my_df_name <- paste0("vesselinfo_raw_", my_username)

DBI::dbWriteTable(conn = con, # the connection
                  name = my_df_name, # the name of the table you will create in the DB
                  value = vesselinfo_raw,
                  overwrite = TRUE)
```

## Task 5 - Get and write the other data sets

::: callout-note
This task is similar to Tasks 2-4 from a Workshop perspective. We will speed through this code and pick up again on the next Task for new material.
:::

Get the following additional data sets:

-   **Vessel History**: the `https://www.wsdot.wa.gov/Ferries/API/Vessels/rest/vesselhistory` endpoint contains historical data about sailings.

-   **Terminal locations**: the `https://www.wsdot.wa.gov/Ferries/API/terminals/rest/terminallocations` endpoint contains information about ferry terminals locations.

-   **Weather data**: the `https://archive-api.open-meteo.com/v1/archive?{params}` endpoint contains historical hourly weather information for a location and date, including things that may affect the timeliness of our ferry departure, such as wind speed, cloud cover, and precipitation.

### Vessel history

We'll query 30 days looking back from today's date, and use the vessel names we retrieved from the Vessel Verbose endpoint.

Rather than manually repeating the API call for each vessel though, we'll use a function we wrote into the `ferryfairy` package that iterates the API call over the list of vessel names using the same technique as above with `httr2`.

```{r}
#| label: download raw vessel history

# Specify our parameters:
start_date <- today()-30
end_date <- today()
vesselnames <- vesselinfo_raw |> pull(vessel_name)

# Use `ferryfairy::get_vesselhistory` to iterate over all of the vessels. The results will be a list, with each vessel's history as a list item. 
data_list <- map(vesselnames, get_vesselhistory, start_date, end_date)

# Convert the list into a data frame and clean the column names
vesselhistory_raw <- bind_rows(data_list) |> clean_names()
vesselhistory_raw
```

### Terminal locations

Again, to simplify things, we use a function in `ferryfairy` to retrieve latitude and longitudes for each terminal. The function is a wrapper around `httr2`.

```{r}
#| label: download terminal locations data

terminallocations <- ferryfairy::get_terminalinfo()
terminallocations
```

### Weather data

Similar to the approach used for retrieving Vessel History, we use a function in `ferryfairy` to call the weather API and iterate over all of the departing terminal locations.

The weather API allows us to specify the data and format we want in the query. We will include:

-   precipitation (inches)

-   weather code

-   cloud cover at 10 m

-   wind speed at 10 m (as mph)

-   wind gusts at 10 m (as mph)

-   Timezone: US/Pacific

The Open Meteo API is available to use without an API key when used for educational purposes according to their [Terms of Use](https://open-meteo.com/en/terms). As such, queries are limited to less than 10,000 API calls per day, 5,000 per hour and 600 per minute.

```{r}
#| label: download weather data

# Use `purrr::map` to iterate over all of the terminals. The results will be a list, with each terminal as a list item. 
data_list <- map(terminallocations$terminal_name, 
                 ferryfairy::get_terminal_weather_history, 
                 start_date, end_date)

# Convert the list into a data frame and clean the column names
weather_terminal_history_raw <- bind_rows(data_list) 
weather_terminal_history_raw

```

### Write the data to the database

We'll use a helper function from `{ferryfairy}` for brevity. This is a wrapper around `dbWriteTable`.

```{r}
#| label: write remaining data to database

write_df_to_db(vesselhistory_raw, paste0("vesselhistory_raw_",my_username), con, append = TRUE)

write_df_to_db(terminallocations, paste0("terminallocations_",my_username), con, overwrite = TRUE)

write_df_to_db(weather_terminal_history_raw, paste0("weather_terminal_history_raw_",my_username), con, append = TRUE)

# We're done here with the database, so we can close the connection
dbDisconnect(con)

```

::: {.callout-tip icon="false"}
## Heads Up

If you're working through this document in the Workshop, pause here.
:::

## Task 6 - Send an email alert if something is fishy üêü

üîÑ Task

We will publish this document and schedule it to run daily. However, what if there's something wrong with the data? We want to know about it!

Posit Connect has support for sending emails with Quarto: <https://docs.posit.co/connect/user/quarto/#email-customization>.

The email subject and body are enclosed within `:::` divs and we specify that this document should render an email in the Quarto YAML block at the top of the document using `format: email`.

üí° Consider `format: email` as an extension of the more general `format: html`. This means other YAML options pertinent to html outputs like `toc` and `code-fold` can still be used.

-   Assume that if fewer than 10 records are returned from any of the API calls, there's an issue with the upstream data source and we want to receive an email alert about it.
-   Do this by setting `send_email` to `TRUE` within a block called `::: {email-scheduled}`. When the boolean is true, the email will be sent; otherwise, it will be suppressed. See <https://docs.posit.co/connect/user/quarto/#suppressing-email>

::: callout
‚ùó‚ùó When viewing this portion of the code in the Workshop, switch from "Visual Editor" mode to "Source Mode" to more easily see the Quarto divs that control the email generation.

![](mode_switch.png)
:::

### Define conditionality

```{r}
#| label: set conditions for sending email

records_threshold <- 10

if(nrow(vesselinfo_raw) < records_threshold | 
   nrow(vesselhistory_raw) < records_threshold | 
   nrow(terminallocations) < records_threshold | 
   nrow(weather_terminal_history_raw) < records_threshold){
  send_email <- TRUE
} else send_email <- FALSE
```

### Compose email

The email below will send if the alert condition is set.

::::: email
::: email-scheduled
```{r}
send_email
```
:::

::: subject
‚ö†Ô∏è Ferry Project: There was an issue with the raw data.
:::

Fewer rows of raw data were retrieved than expected. The number of records are:

```{r}
glue("The following number of records were retrieved from the API sources: 
     
     {nrow(vesselinfo_raw)} rows of raw vessel data
     {nrow(vesselhistory_raw)} rows of raw vessel history
     {nrow(terminallocations)} terminal location records
     {nrow(weather_terminal_history_raw)} rows of weather data
     
     A minumum of {records_threshold} records are expected from each source. Please review the source data.")

```
:::::

## Task 7 - Add logging information to make our scheduled report informative

üîÑ Task

Make your scheduled reports work harder for you! Include information that you'll find useful to refer back to. Logging can be as basic or elaborate as you need.

Lets include:

-   A nicely formatted time stamp.

-   A summary the main actions taken in the report, such as how many rows of data were downloaded, how many rows of data were written to the database

Hint: The `glue` package is helpful for combining text and variables. It's less work than stringing things together with `paste`. With `glue::glue()`, the syntax is `glue("Text with {<r expression or variable>}")`

```{r}
#| label: logging

stamp <- format(as.POSIXct(Sys.time(),tz="US/Pacific"),'%A, %B %d, %Y %H:%M:%S')
glue("Report run {stamp}")

glue("Wrote the following to the database: 
     
     {nrow(vesselinfo_raw)} rows of raw vessel data
     {nrow(vesselhistory_raw)} rows of raw vessel history
     {nrow(terminallocations)} terminal location records
     {nrow(weather_terminal_history_raw)} rows of weather data")

```

## Task 8 - Publish and schedule on Posit Connect

üîÑ Task

-   Connect your account on Posit Connect to Workbench (Tools \> Global Options \> Publishing)
-   Use push-button publishing to publish this document, with source code, to Connect
-   Schedule it to run on a timescale that seems appropriate for the data update cycle

Note: Recall we specified our database connection details with environment variables. Typically when you publish content to Connect, you will also need to supply the environment variables using the "Vars" pane in Connect. For this workshop, these environment variables have already been stored on the Connect server for you so you won't need to do this step for this activity.

We'll do this task together as a group.

For reference, the Connect User Guide provides instructions for publishing: <https://docs.posit.co/connect/user/publishing/#publishing-general>
